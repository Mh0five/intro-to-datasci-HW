{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"import requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\nnltk.download('stopwords')\n\nurl = 'https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm'\nr = requests.get(url)\nhtml = r.content.decode('utf-8')\n\nhtml_soup = BeautifulSoup(html, 'html.parser')\nmoby_text = html_soup.get_text()\ntokenizer = RegexpTokenizer(r'\\w+')\nwords = [word.lower() for word in tokenizer.tokenize(moby_text)]\n\n\nstop_words = stopwords.words('english')\nwords_no_stop = [word for word in words if word not in stop_words]\n\n\ncount = Counter(words_no_stop)\ntop_ten = word_count.most_common(10)\n\ntop_ten","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":893,"lastSuccessfullyExecutedCode":"import requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\nnltk.download('stopwords')\n\nurl = 'https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm'\nr = requests.get(url)\nhtml = r.content.decode('utf-8')\n\nhtml_soup = BeautifulSoup(html, 'html.parser')\nmoby_text = html_soup.get_text()\ntokenizer = RegexpTokenizer(r'\\w+')\nwords = [word.lower() for word in tokenizer.tokenize(moby_text)]\n\n\nstop_words = stopwords.words('english')\nwords_no_stop = [word for word in words if word not in stop_words]\n\n\ncount = Counter(words_no_stop)\ntop_ten = word_count.most_common(10)\n\ntop_ten","executionCancelledAt":null,"lastExecutedAt":1764205540739,"lastExecutedByKernel":"2561ce5f-b0bf-4947-8cb3-a1d475210a7a","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"[('whale', 1246),\n ('one', 925),\n ('like', 647),\n ('upon', 568),\n ('man', 527),\n ('ship', 519),\n ('ahab', 517),\n ('ye', 473),\n ('sea', 455),\n ('old', 452)]"},"metadata":{},"execution_count":46}]},{"source":"","metadata":{},"cell_type":"markdown","id":"f97c18cb-7b5d-4076-a922-c9c50455e500"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}